Introduction and Analysis of Sorting Algorithms


------------Selection Sort-----------
*Introduction: A simple sorting algorithm that repeatedly searches remaining items to find the least one and moves it to its final location[1], which mirrors the way we often sort items by hand.
*Algorithm Steps:
step 1: Go through the unsorted part of the array to find the smallest element (In the first run, sorted part's number of element is 0)
step 2: Swap the smallest element found in step 1 with the first element behind the sorted part
step 3: Move the boundary between the sorted part and the unsorted part by one position toward the end of the array
step 3: If the number of elements in the sorted part is not equal to the number of elements in the array, go back to step 1. Otherwise, the array is sorted
*Complexity Analysis: As selection sort does not take advantage of the array's distribution and always performs the same operations, the complexity remains the same in the best-case and worst-case scenarios. 
    Best-case time complexity: O(n^2)
    Average-case time complexity: O(n^2)
    Worst-case time complexity: O(n^2)

------------Insertion Sort------------
*Introduction: A simple sorting algorithm that sorts by repeatedly taking the next item and inserting it into the final data structure in its proper order with respect to items already inserted[2].
*Algorithm Steps:
step 1: Pick the first element (As there's just one element, the number of elements in the sorted part will be 1.)
step 2: Pick the first element behind the sorted part
step 3: Compare it with each element in the sorted part to find the target position
step 4: Insert the element into the sorted part at the position found in step 3
step 5: Move the boundary between sorted part and unsorted part by one position toward the end of array
step 6: If the number of sorted part's element is not equal to the number of element in the array, go back to step 2. Otherwise, the array is sorted.
*Complexity Analysis: 
    Best-case time complexity: O(n) - happens when the array is already sorted
    Average-case time complexity: O(n^2)
    Worst-case time complexity: O(n^2)
*Optimizations: When comparisons are expensive, we can use binary search instead of comparing each element to find the position in step 3. However, this would make the sort unstable when there are duplicate elements.

------------Bubble Sort------------
*Introduction:
*Algorithm Steps:
*Complexity Analysis:
*Optimizations:

------------Heap Sort------------
*Introduction: A sorting algorithm that heapifies the array to find the largest value of the unsorted part of the array (max-heap) and bring it to the end of the array.
*Algorithm Steps:
step 1: Construct a max-heap for the array
	Condition: arr[i] > arr[2i + 1] and arr[i] > arr[2i + 2] (Condition does not apply to the second half as 2i+1 and 2i+2 are 	out of array)
step 2: Swap the root (arr[0]) with the last element of the heap
step 3: Reduce the size of the heap by 1
step 4: If the size of the heap is greater than 0, go to step 1. Otherwise, end the process since the array is fully sorted.
*Complexity Analysis:
    Best-case time complexity: O(nlog2(n))
    Average-case time complexity: O(nlog2(n))
    Worst-case time complexity: O(nlog2(n))

------------Merge Sort------------
*Introduction: Merge sort is an algorithm that follows the divide-and-conquer method. Starting with the entire array A[1:n], it recurses down to smaller and smaller subarrays. The key operation of the merge sort algorithm occurs in the "combine" step, which merges two adjacent, sorted subarrays (Cormen et al, 2022, pp. 35-37).
*Algorithm Steps:
step 1: If the input array has only 1 element, return that input array as sorted. If the input array has more than 1 element, continue to step 2.
step 2: Split the input array into 2 smaller arrays, the first subarray (subarray 1) takes the original array's first half number of elements, the second subarray (subarray 2) takes the remain elements in the input array.
step 3: Perform step 1-3 for each of the two subarrays as input arrays.
step 4: Compare the first untaken element of subarray 1 with the first untaken element of subarray 2, and add the smaller element to the input array to the first position of unsorted part. If either sub-array has no untaken element left, add the first untaken element of the other sub-array to the input array.
step 5: Increate the number of taken element of the sub-array that provided an element by 1, increase the number of sorted element in the input array by 1.
step 6: If the number of elements in the sorted part is not equal to the number of elements of the input array, go back to step 4. Otherwise, the input array is sorted.
*Complexity Analysis: Because merge sort splits the entire n-array to n 1-element arrays then merges them to sort, the complexity does not depend on the array's distribution.
    Best-case time complexity: O(nlog(n))
    Average-case time complexity: O(nlog(n))
    Worst-case time complexity: O(nlog(n))

------------Quick Sort------------
*Introduction: Quick sort applies the divide-and-conquer method. The general idea is choosing an element to be the pivot, then take all the smaller element than pivot to the left, and take all larger elements than pivot to the right; and then do the same for those 2 subarrays recursively (Cormen et al, 2022, pp. 184-188).
*Algorithm Steps:
step 1:  If there's only 1 element, the array is sorted. Otherwise, choose randomly one element in the input array to be the pivot, swap it with the last element in the array.
step 2: Compare the first unordered element with the pivot, if it is smaller than the pivot, swap it with first larger element (which starts with the first element). Repeat this step until ordering all the elements except the pivot.
step 3: Swap the pivot with the first larger element. If all the elements are smaller than the pivot, do nothing.
step 4: Repeat steps 1-3 with 2 subarrays on the left/right of the pivot.
*Complexity Analysis: Quick sort's performance depends greatly on how we choose the pivot and array's distribution.
    Best-case time complexity: O(nlog(n)) - happens when we always pick the pivot to be the median of the array, then 2 subarrays will be 2 perfect halves; hence we will recurse log(n) times
    Average-case time complexity: O(nlog(n)) - depends on the chosen pivot are good or bad, but on average, a good split will eventually absorb a bad split hence the complexity will be O(nlog(n)) with some hidden contants (Cormen et al, 2022, pp. 191).
    Worst-case time complexity: O(n^2) - happens when we always pick the pivot to be either the smallest or the largest element in the array, then we will have to recurse n times
*Optimizations: We can use Tukey's ninther for better pivot selection, or combining it with insertion sort to handle small subarrays efficiently, or employing Hoare’s partition scheme for improved partitioning, and we can also use three-way partitioning to efficiently handle arrays with many duplicate keys.

------------Radix Sort------------
*Introduction:
*Algorithm Steps:
*Complexity Analysis:
*Optimizations:

------------Shaker Sort------------
*Introduction:
*Algorithm Steps:
*Complexity Analysis:
*Optimizations:

------------Shell sort------------
*Introduction: An algorithm bases on Insertion Sort; proceeds left to right through a file, inserting each element into position among the elements to its left (which are in sorted order) by moving the larger ones one position to the right[5].
*Algorithm Steps[4]: Records R_1,..., R_N are rearranged in place; after sorting is complete, their keys will be in order, K_1 ≤ · · · ≤ K_N. An auxiliary sequence of increments h_t−1, h_t−2, . . ., h0 is used to control the sorting process, where h_0 = 1;
step 1: [Loop on s.] Perform step 2 for s = t − 1, t − 2, . . ., 0; then terminate the algorithm.
step 2: [Loop on j.] Set h ← h_s, and perform steps 3 through 6 for h < j ≤ N.
step 3: [Set up i, K, R.] Set i ← j − h, K ← K_j, R ← R_j.
step 4: [Compare K : Ki.] If K ≥ K_i, go to step 6.
step 5: [Move Ri, decrease i.] Set R_i+h ← R_i, then i ← i − h. If i > 0, go back to step 4.
step 6: [R into Ri+h.] Set R_i+h ← R
*Complexity Analysis: The complexity depends on the sequence h_t−1, h_t−2, . . ., h_0 above, with the Shell's original sequence: n/2, n/4, . . ., 1; the complexity would be:
    Best-case time complexity: O(nlog(n)) - when the array is already sorted
    Average-case time complexity: O(nlog(n))
    Worst-case time complexity: O(n^2)
*Optimizations: The best-known sequence of increments for the Shell sort algorithm is Marcin Ciura's gap sequence[6], detailed in Marcin Ciura's work on "Best Increments for the Average Case of Shellsort" (Fundamentals of Computation Theory: 13th International Symposium, FCT 2001).

------------Counting Sort------------
*Introduction: Counting Sort is a sorting algorithm assumes that each of the n input elements is an integer in the range 0 to k, for some integer k. It first determines, for each input element x , the number of elements less than or equal to x . It then uses this information to place element x directly into its position in the output array.(Cormen et al, 2022, pp. 208-209)
*Algorithm Steps:
step 1: Find the maximum value K in the input arrays A[1:n]
step 2: Create 2 arrays B and C; B has the same size with the input array (B[1:n]), C has size K+1 (C[0:K]). Initialize array C with 0.
step 3: Count the frequency of each distinct number x in the input array, put them in C[x]
step 4: Sum up those frequencies of numbers smaller than m and frequency of m and put it into C[x]
step 5: [Loop on j.] Perform step 6 for j = n, n - 1, . . ., 1; then B is the output sorted array
step 6: The frequency of number in A[j] (which is stored in C[A[j]]) is position of A[j] in the output array B, minus that frequency by 1.
*Complexity Analysis: With k being the largest number in the input array; because the algorithm always go through loop on n twice and loop on k twice, the time complexity will always be O(n+k)

------------Flash Sort------------
*Introduction: Flash Sort consists of three logical blocks: classification, permutation, and straight insertion. Classification determines the size of each class of elements. Permutation does long-range reordering to collect elements of each class together, and straight insertion does the final short-range ordering[7].
*Algorithm Steps:
step 1: [Start of classification] Find the value of smallest element and the index of the largest element, if the largest and the smallest numbers are equal, all the elements are identical. Hence, the array is sorted.
step 2: Create an array L to store the number of elements in each "bucket" (the number of bucket is usually determined by m = 0.45 * n with n being the number of elements in the input array) initialize the a array with 0
step 3: Determine the bucket of each element (by the formula: floor( ((m - 1.0)/(a[nmax]-min)) * (a[i] - min) ); with nmax: the largest number's index; min: the smallest number's value; i: running number to go through the whole array a; function floor is to get the largest natural number smaller than the number inside), count and store them in array L
step 4: [End of classification] Convert the number of elements in each bucket into the accumulative sum in the array L
step 5: [Permutation] Swapping to put elements into its target bucket
step 6: [Straight insertion] Use Insertion Sort to sort elements in each bucket 
*Complexity Analysis:
    Average-case time complexity: O(n) - when the array has a known distribution.
    Worst-case time complexity: O(n^2) - when the distribution of the elements in the input array is highly non-uniform, which can lead to bad bucket distributions.
*Optimizations: We can use resursion in "Straight insertion" block to continue sorting elements in a bucket if the number of elements in a bucket is not small the enough to hide the ineffient of Insertion Sort.


References:
[1]: Paul E. Black, "selection sort", in Dictionary of Algorithms and Data Structures [online], Paul E. Black, ed. 21 April 2022, https://www.nist.gov/dads/HTML/selectionSort.html. Accessed 26 June 2024.
[2]: Paul E. Black, "insertion sort", in Dictionary of Algorithms and Data Structures [online], Paul E. Black, ed. 6 April 2023,https://www.nist.gov/dads/HTML/insertionSort.html. Accessed 26 June 2024.
[3]: Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2022). Introduction to algorithms (4th ed.). MIT Press.
[4]: Knuth, D.E., 1998. The Art of Computer Programming. 2nd ed. Vol. 3. Reading, MA: Addison-Wesley, pp. 84.
[5]: Diaz, J. and Serna, M. (eds), 1996. Algorithms - ESA 1996 - 4th Annual European Symposium, Proceedings. Barcelona, Spain: Springer Verlag, pp. 1.
[6]: Wikipedia contributors, n.d. Shellsort. Wikipedia, The Free Encyclopedia,https://en.wikipedia.org/wiki/Shellsort. Accessed: 28 June 2024.
[7]: Neubert, K.-D., 1998. The Flashsort1 Algorithm. Dr. Dobb's Journal, February,https://www.neubert.net/FSOIntro.html. Accessed 30 June 2024.